{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzCOjjP4caiYGLdteF/zBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnikolovskiii/Deep-learning-lab-exercises/blob/master/Citation_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=\"lightgreen\">Citation network dataset (benchmark dataset)</font>"
      ],
      "metadata": {
        "id": "An5hTLL_aCy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph: The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers indexed by MAG [1]. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the skip-gram model [2] over the MAG corpus. We also provide the mapping from MAG paper IDs into the raw texts of titles and abstracts here. In addition, all papers are also associated with the year that the corresponding paper was published."
      ],
      "metadata": {
        "id": "0MxKP_9TX8ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link: https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv"
      ],
      "metadata": {
        "id": "GAyGDhVQYAcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDDfnCJfRrZ7",
        "outputId": "73b30627-af2a-413e-ae45-31df00567db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m0.8/1.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc6OjheER3QZ",
        "outputId": "74a8a4ee-7c45-4236-b8d0-f81ad866e321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=fa4e3de4d9aa684633702f2cd44ebfc3e3268db566713a2a34e009bf1155d080\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running everything on GPU"
      ],
      "metadata": {
        "id": "YMYKZWJBYPgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "        print(\"GPU is available.\")\n",
        "else:\n",
        "        print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IhRXTCiU3rX",
        "outputId": "acbe5b2f-725b-432d-d306-4d7260a45def"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=\"lightgreen\">GCN Model</font>"
      ],
      "metadata": {
        "id": "CbRbth5QYdoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the image it is shown the layers that are a part od this Graph convolutional neural network. The output layer is a soft classification, and the lost function is negative log likelihood, where it takes into account all of the predicted classes, and compares it to the target label."
      ],
      "metadata": {
        "id": "5mNrUSoBYjwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained in 100 epochs. It is trained on the whole data, but it is trained, tested and evaluated, on a certain set of data. In the end the best model is saved, which has best score in validation phase."
      ],
      "metadata": {
        "id": "8MGpcXjYZU-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has learned embeddings for the nodes in our netork and the next task will be to use the for classificaion!"
      ],
      "metadata": {
        "id": "UI1nF97QZ3e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)"
      ],
      "metadata": {
        "id": "QEU3gTDsYabF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq0tO1BbRgXY",
        "outputId": "f6fba95c-b4c5-491b-88ba-261aa344ef64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n",
            "PygNodePropPredDataset()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.0895, Train: 6.05%, Valid: 4.58% Test: 3.86%\n",
            "Epoch: 02, Loss: 2.9380, Train: 15.05%, Valid: 15.23% Test: 14.09%\n",
            "Epoch: 03, Loss: 2.5421, Train: 20.30%, Valid: 25.13% Test: 24.05%\n",
            "Epoch: 04, Loss: 2.3887, Train: 30.44%, Valid: 36.20% Test: 33.10%\n",
            "Epoch: 05, Loss: 2.2684, Train: 37.07%, Valid: 42.43% Test: 40.27%\n",
            "Epoch: 06, Loss: 2.1728, Train: 38.66%, Valid: 44.01% Test: 42.51%\n",
            "Epoch: 07, Loss: 2.1057, Train: 38.43%, Valid: 43.51% Test: 43.28%\n",
            "Epoch: 08, Loss: 2.0463, Train: 38.68%, Valid: 43.71% Test: 43.83%\n",
            "Epoch: 09, Loss: 1.9987, Train: 38.84%, Valid: 44.06% Test: 44.57%\n",
            "Epoch: 10, Loss: 1.9484, Train: 39.39%, Valid: 44.64% Test: 45.14%\n",
            "Epoch: 11, Loss: 1.9131, Train: 40.44%, Valid: 45.18% Test: 45.69%\n",
            "Epoch: 12, Loss: 1.8858, Train: 42.10%, Valid: 45.84% Test: 46.28%\n",
            "Epoch: 13, Loss: 1.8618, Train: 44.66%, Valid: 47.64% Test: 47.01%\n",
            "Epoch: 14, Loss: 1.8333, Train: 47.82%, Valid: 49.82% Test: 47.67%\n",
            "Epoch: 15, Loss: 1.8058, Train: 50.49%, Valid: 51.43% Test: 48.00%\n",
            "Epoch: 16, Loss: 1.7825, Train: 52.48%, Valid: 52.43% Test: 47.77%\n",
            "Epoch: 17, Loss: 1.7615, Train: 53.89%, Valid: 53.17% Test: 47.29%\n",
            "Epoch: 18, Loss: 1.7478, Train: 54.81%, Valid: 53.44% Test: 46.80%\n",
            "Epoch: 19, Loss: 1.7346, Train: 55.36%, Valid: 53.65% Test: 46.49%\n",
            "Epoch: 20, Loss: 1.7264, Train: 55.70%, Valid: 53.79% Test: 46.52%\n",
            "Epoch: 21, Loss: 1.7133, Train: 55.99%, Valid: 54.03% Test: 46.60%\n",
            "Epoch: 22, Loss: 1.6931, Train: 56.14%, Valid: 54.23% Test: 46.75%\n",
            "Epoch: 23, Loss: 1.6815, Train: 56.35%, Valid: 54.37% Test: 46.99%\n",
            "Epoch: 24, Loss: 1.6673, Train: 56.58%, Valid: 54.39% Test: 46.89%\n",
            "Epoch: 25, Loss: 1.6592, Train: 56.74%, Valid: 54.18% Test: 46.66%\n",
            "Epoch: 26, Loss: 1.6500, Train: 56.84%, Valid: 54.00% Test: 46.57%\n",
            "Epoch: 27, Loss: 1.6432, Train: 56.91%, Valid: 54.03% Test: 46.63%\n",
            "Epoch: 28, Loss: 1.6347, Train: 56.96%, Valid: 53.97% Test: 46.52%\n",
            "Epoch: 29, Loss: 1.6251, Train: 57.01%, Valid: 53.85% Test: 46.48%\n",
            "Epoch: 30, Loss: 1.6145, Train: 57.12%, Valid: 53.85% Test: 46.47%\n",
            "Epoch: 31, Loss: 1.6116, Train: 57.34%, Valid: 53.78% Test: 46.53%\n",
            "Epoch: 32, Loss: 1.5995, Train: 57.51%, Valid: 53.95% Test: 46.64%\n",
            "Epoch: 33, Loss: 1.5926, Train: 57.83%, Valid: 54.27% Test: 47.02%\n",
            "Epoch: 34, Loss: 1.5884, Train: 58.14%, Valid: 54.69% Test: 47.72%\n",
            "Epoch: 35, Loss: 1.5845, Train: 58.43%, Valid: 55.20% Test: 48.22%\n",
            "Epoch: 36, Loss: 1.5755, Train: 58.63%, Valid: 55.58% Test: 48.64%\n",
            "Epoch: 37, Loss: 1.5648, Train: 58.74%, Valid: 55.87% Test: 48.98%\n",
            "Epoch: 38, Loss: 1.5618, Train: 58.78%, Valid: 55.96% Test: 49.12%\n",
            "Epoch: 39, Loss: 1.5577, Train: 58.91%, Valid: 56.06% Test: 49.19%\n",
            "Epoch: 40, Loss: 1.5499, Train: 59.01%, Valid: 56.21% Test: 49.51%\n",
            "Epoch: 41, Loss: 1.5465, Train: 59.18%, Valid: 56.53% Test: 49.85%\n",
            "Epoch: 42, Loss: 1.5416, Train: 59.38%, Valid: 56.62% Test: 50.11%\n",
            "Epoch: 43, Loss: 1.5352, Train: 59.43%, Valid: 56.61% Test: 50.13%\n",
            "Epoch: 44, Loss: 1.5308, Train: 59.46%, Valid: 56.53% Test: 50.02%\n",
            "Epoch: 45, Loss: 1.5290, Train: 59.42%, Valid: 56.32% Test: 49.61%\n",
            "Epoch: 46, Loss: 1.5237, Train: 59.42%, Valid: 56.05% Test: 49.34%\n",
            "Epoch: 47, Loss: 1.5178, Train: 59.49%, Valid: 56.00% Test: 49.31%\n",
            "Epoch: 48, Loss: 1.5171, Train: 59.74%, Valid: 56.33% Test: 49.65%\n",
            "Epoch: 49, Loss: 1.5117, Train: 60.10%, Valid: 56.87% Test: 50.26%\n",
            "Epoch: 50, Loss: 1.5086, Train: 60.27%, Valid: 57.37% Test: 50.94%\n",
            "Epoch: 51, Loss: 1.5027, Train: 60.40%, Valid: 57.66% Test: 51.27%\n",
            "Epoch: 52, Loss: 1.5001, Train: 60.52%, Valid: 57.70% Test: 51.26%\n",
            "Epoch: 53, Loss: 1.4954, Train: 60.52%, Valid: 57.53% Test: 51.08%\n",
            "Epoch: 54, Loss: 1.4907, Train: 60.58%, Valid: 57.49% Test: 50.94%\n",
            "Epoch: 55, Loss: 1.4891, Train: 60.66%, Valid: 57.49% Test: 50.95%\n",
            "Epoch: 56, Loss: 1.4883, Train: 60.70%, Valid: 57.44% Test: 51.07%\n",
            "Epoch: 57, Loss: 1.4786, Train: 60.74%, Valid: 57.61% Test: 51.30%\n",
            "Epoch: 58, Loss: 1.4807, Train: 60.82%, Valid: 57.83% Test: 51.51%\n",
            "Epoch: 59, Loss: 1.4766, Train: 60.92%, Valid: 57.85% Test: 51.51%\n",
            "Epoch: 60, Loss: 1.4752, Train: 60.99%, Valid: 57.88% Test: 51.63%\n",
            "Epoch: 61, Loss: 1.4696, Train: 61.12%, Valid: 57.93% Test: 51.57%\n",
            "Epoch: 62, Loss: 1.4695, Train: 61.23%, Valid: 57.94% Test: 51.40%\n",
            "Epoch: 63, Loss: 1.4603, Train: 61.30%, Valid: 57.88% Test: 51.37%\n",
            "Epoch: 64, Loss: 1.4620, Train: 61.39%, Valid: 58.05% Test: 51.56%\n",
            "Epoch: 65, Loss: 1.4562, Train: 61.48%, Valid: 58.29% Test: 51.80%\n",
            "Epoch: 66, Loss: 1.4572, Train: 61.56%, Valid: 58.39% Test: 52.06%\n",
            "Epoch: 67, Loss: 1.4514, Train: 61.58%, Valid: 58.70% Test: 52.41%\n",
            "Epoch: 68, Loss: 1.4533, Train: 61.64%, Valid: 58.80% Test: 52.52%\n",
            "Epoch: 69, Loss: 1.4477, Train: 61.73%, Valid: 58.68% Test: 52.42%\n",
            "Epoch: 70, Loss: 1.4449, Train: 61.80%, Valid: 58.50% Test: 52.05%\n",
            "Epoch: 71, Loss: 1.4407, Train: 61.90%, Valid: 58.36% Test: 51.90%\n",
            "Epoch: 72, Loss: 1.4413, Train: 61.97%, Valid: 58.40% Test: 51.94%\n",
            "Epoch: 73, Loss: 1.4374, Train: 62.07%, Valid: 58.55% Test: 52.19%\n",
            "Epoch: 74, Loss: 1.4349, Train: 62.11%, Valid: 58.70% Test: 52.37%\n",
            "Epoch: 75, Loss: 1.4319, Train: 62.15%, Valid: 59.00% Test: 52.78%\n",
            "Epoch: 76, Loss: 1.4311, Train: 62.19%, Valid: 59.02% Test: 52.84%\n",
            "Epoch: 77, Loss: 1.4273, Train: 62.23%, Valid: 59.11% Test: 52.96%\n",
            "Epoch: 78, Loss: 1.4285, Train: 62.32%, Valid: 59.26% Test: 53.08%\n",
            "Epoch: 79, Loss: 1.4239, Train: 62.31%, Valid: 59.19% Test: 53.20%\n",
            "Epoch: 80, Loss: 1.4223, Train: 62.36%, Valid: 59.07% Test: 52.96%\n",
            "Epoch: 81, Loss: 1.4214, Train: 62.40%, Valid: 59.07% Test: 52.88%\n",
            "Epoch: 82, Loss: 1.4167, Train: 62.44%, Valid: 59.10% Test: 53.01%\n",
            "Epoch: 83, Loss: 1.4110, Train: 62.50%, Valid: 59.21% Test: 53.27%\n",
            "Epoch: 84, Loss: 1.4099, Train: 62.54%, Valid: 59.32% Test: 53.24%\n",
            "Epoch: 85, Loss: 1.4116, Train: 62.64%, Valid: 59.10% Test: 53.10%\n",
            "Epoch: 86, Loss: 1.4101, Train: 62.78%, Valid: 59.07% Test: 52.90%\n",
            "Epoch: 87, Loss: 1.4085, Train: 62.88%, Valid: 59.15% Test: 53.06%\n",
            "Epoch: 88, Loss: 1.4077, Train: 62.83%, Valid: 59.45% Test: 53.48%\n",
            "Epoch: 89, Loss: 1.3999, Train: 62.83%, Valid: 59.57% Test: 53.78%\n",
            "Epoch: 90, Loss: 1.4046, Train: 62.88%, Valid: 59.63% Test: 53.70%\n",
            "Epoch: 91, Loss: 1.3999, Train: 63.05%, Valid: 59.35% Test: 53.35%\n",
            "Epoch: 92, Loss: 1.3935, Train: 63.07%, Valid: 59.30% Test: 53.10%\n",
            "Epoch: 93, Loss: 1.3944, Train: 63.10%, Valid: 59.40% Test: 53.40%\n",
            "Epoch: 94, Loss: 1.3933, Train: 63.11%, Valid: 59.59% Test: 53.74%\n",
            "Epoch: 95, Loss: 1.3901, Train: 63.15%, Valid: 59.64% Test: 53.89%\n",
            "Epoch: 96, Loss: 1.3904, Train: 63.22%, Valid: 59.56% Test: 53.75%\n",
            "Epoch: 97, Loss: 1.3861, Train: 63.36%, Valid: 59.58% Test: 53.47%\n",
            "Epoch: 98, Loss: 1.3856, Train: 63.42%, Valid: 59.57% Test: 53.39%\n",
            "Epoch: 99, Loss: 1.3817, Train: 63.44%, Valid: 59.52% Test: 53.46%\n",
            "Epoch: 100, Loss: 1.3832, Train: 63.41%, Valid: 59.71% Test: 53.82%\n",
            "Saving Model Predictions\n",
            "Best model: Train: 63.41%, Valid: 59.71% Test: 53.82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "print(torch.__version__)\n",
        "import copy\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList([GCNConv(input_dim, hidden_dim)] +\n",
        "                                   [GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)]+\n",
        "                                   [GCNConv(hidden_dim, output_dim)])\n",
        "\n",
        "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "        # The log softmax layer\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            if i < len(self.bns):\n",
        "                x = self.bns[i](x)\n",
        "                x = torch.relu(x)\n",
        "                x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        if self.return_embeds:\n",
        "            return x\n",
        "        else:\n",
        "            x = F.log_softmax(x, dim=1)\n",
        "            return x\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "    # Slice the model's output and target labels using train_idx\n",
        "    train_output = out[train_idx]\n",
        "    train_target = data.y[train_idx]\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(train_output, train_target.flatten())\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model's weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return the loss as a float\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass: compute the model's predictions on the entire dataset\n",
        "    out = model(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "        print(\"Saving Model Predictions\")\n",
        "\n",
        "        data = {'y_pred': y_pred.view(-1).cpu().detach().numpy()}\n",
        "\n",
        "        df = pd.DataFrame(data=data)\n",
        "        # Save locally as csv\n",
        "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    dataset_name = 'ogbn-arxiv'\n",
        "    data = PygNodePropPredDataset(name=dataset_name)\n",
        "    data.x = data.x.to(device)\n",
        "    data.y = data.y.to(device)\n",
        "\n",
        "    split_idx = data.get_idx_split()\n",
        "    train_loader = DataLoader(split_idx[\"train\"], batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(split_idx[\"valid\"], batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(split_idx[\"test\"], batch_size=32, shuffle=False)\n",
        "\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "    tmp = data.num_classes\n",
        "\n",
        "\n",
        "    args = {\n",
        "        'num_layers': 3,\n",
        "        'hidden_dim': 256,\n",
        "        'dropout': 0.5,\n",
        "        'lr': 0.01,\n",
        "        'epochs': 100,\n",
        "        'device' : device\n",
        "    }\n",
        "\n",
        "\n",
        "    model = GCN(data.num_features, args['hidden_dim'],\n",
        "                data.num_classes, args['num_layers'],\n",
        "                args['dropout'])\n",
        "    model = model.to(device)\n",
        "    evaluator = Evaluator(name='ogbn-arxiv')\n",
        "\n",
        "    # reset the parameters to initial random value\n",
        "    model.reset_parameters()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    loss_fn = F.nll_loss\n",
        "\n",
        "    best_model = None\n",
        "    best_valid_acc = 0\n",
        "    print(data)\n",
        "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "        loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "        result = test(model, data, split_idx, evaluator)  # Evaluate on the validation set\n",
        "        train_acc, valid_acc, test_acc = result\n",
        "        if valid_acc > best_valid_acc:\n",
        "            best_valid_acc = valid_acc\n",
        "            best_model = copy.deepcopy(model)\n",
        "        print(f'Epoch: {epoch:02d}, '\n",
        "              f'Loss: {loss:.4f}, '\n",
        "              f'Train: {100 * train_acc:.2f}%, '\n",
        "              f'Valid: {100 * valid_acc:.2f}% '\n",
        "              f'Test: {100 * test_acc:.2f}%')\n",
        "\n",
        "    best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
        "    train_acc, valid_acc, test_acc = best_result\n",
        "    print(f'Best model: '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    }
  ]
}